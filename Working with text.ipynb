{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Text\n",
    "\n",
    "Text in Python has the data type “string.” That means that text is just a sequence of characters. When analysing text programmatically, we are generally not interested in formatting, but just in the raw text itself. As a consequence, you cannot simply open a Word document or something like that in Python. The text format that we usually work with is `.txt`.\n",
    "\n",
    "Text can be stored in different encodings. These specify how the characters (e.g., “a”, but also “á” and even “道”) are converted to 0 and 1 for the computer. The most modern encoding is UTF-8, which follows the Unicode standard. You should make sure that your texts are always saved using UTF-8 in order to prevent issues when reading them in Python.\n",
    "\n",
    "That said, let’s look how to work with text in Python. In this directory, there is a file called “heimskringla_preface.txt”. It is the beginning of a Norwegian saga available from [Project Gutenberg](http://www.gutenberg.org/ebooks/598).\n",
    "\n",
    "In order to work with the text in python, we have to get it from the file stored on the computer’s harddrive, into a python variable that we can work with. It is good practice not to keep the file open all the time. Since we can do everything we want with the text once it is stored in a variable, we can close the file directly after reading its content. So the steps are simply:\n",
    "\n",
    "1. Open file.\n",
    "2. Read content.\n",
    "3. Close file.\n",
    "4. Analyse text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textfile = open('heimskringla_preface.txt')\n",
    "text = textfile.read()\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more elegant, but a bit less readable variant is to open the file just as long as we need it and let Python close it automatically afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('heimskringla_preface.txt') as textfile:\n",
    "    text = textfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `text` is now a large string, containing the complete content of the file. It wouldn’t make sense to print it completely, so we just peek into its beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3076"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE OF SNORRE STURLASON.\\n\\nIn this book I have had old stories written down, as I have heard\\nthem told by intelligent people, concerning chiefs who have have held\\ndominion in the northern countries'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = text[0:200]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE OF SNORRE STURLASON.\n",
      "\n",
      "In this book I have had old stories written down, as I have heard\n",
      "them told by intelligent people, concerning chiefs who have have held\n",
      "dominion in the northern countries\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete text is often difficult to work with, so we’d want to split it into smaller parts. A starting point might be lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFACE OF SNORRE STURLASON.',\n",
       " '',\n",
       " 'In this book I have had old stories written down, as I have heard',\n",
       " 'them told by intelligent people, concerning chiefs who have have held',\n",
       " 'dominion in the northern countries']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sample_text.splitlines()\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have split the text into pieces, we can start to inspect the text piece by piece. For example, we might want to extract a table of contents in order to get an overview over the text. Since we know that headings are always printed using uppercase characters in this edition, we can go through all the lines and only print those that are headings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE OF SNORRE STURLASON.\n"
     ]
    }
   ],
   "source": [
    "for line in lines:\n",
    "    if line.isupper():\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with text, we often have to clean them up. Some parts of this work require manual investigation, but many parts can be described as simple rules. These are tasks that are perfect for Python. In this case, we might decide to convert the headings from uppercase to the more usual title case, but otherwise leave the text unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preface Of Snorre Sturlason.\n",
      "\n",
      "In this book I have had old stories written down, as I have heard\n",
      "them told by intelligent people, concerning chiefs who have have held\n",
      "dominion in the northern countries\n"
     ]
    }
   ],
   "source": [
    "for line in lines:\n",
    "    if line.isupper():\n",
    "        line = line.title()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we want to transform data this way. And instead of printing it, we want to store the result for further processing. One good way to express such data conversions in Python are so-called *list comprehensions*. They are rules that describe how to transform a list of values into another list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = ['This', 'is', 'a', 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical task for working with data is *mapping,* i.e. transforming each element of a list into another element. If we want to transform each element of this list, e.g. into uppercase, the usual way would be:\n",
    "\n",
    "1. To create a new list (e.g. `test_transformed`) which is empty in the beginning,\n",
    "2. to loop through the original list,\n",
    "3. to transform each element, and\n",
    "4. to append the transformed element to the new list.\n",
    "\n",
    "In order to visualise how the new list gets filled up, I print the state of `test_transformed` after each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS']\n",
      "['THIS', 'IS']\n",
      "['THIS', 'IS', 'A']\n",
      "['THIS', 'IS', 'A', 'TEST']\n"
     ]
    }
   ],
   "source": [
    "test_transformed = []\n",
    "for word in test:\n",
    "    word_transformed = word.upper()\n",
    "    test_transformed.append(word_transformed)\n",
    "    print(test_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shorter version would be using a list comprehension. It dynamically creates a new list from a given list, practically using an embedded loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS', 'IS', 'A', 'TEST']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transformed = [word.upper() for word in test]\n",
    "test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second, typical transformation task is *filtering,* i.e. selecting only those elements from a list that match certain criteria. Here, we want to select only thos words from our example list `test` that have more than two letters. The classical way would be to embed an `if` statement into our loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This']\n",
      "['This']\n",
      "['This']\n",
      "['This', 'test']\n"
     ]
    }
   ],
   "source": [
    "test_filtered = []\n",
    "for word in test:\n",
    "    if len(word) > 2:\n",
    "        test_filtered.append(word)\n",
    "    print(test_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a list comprehension can make this easier by adding the filter criterion directly to the list generation procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'test']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filtered = [word for word in test if len(word) > 2 ]\n",
    "test_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to combine both steps, i.e. to filter list elements and to transform those elements that get included into the new list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Test']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_filtered_transformed = [word.title() for word in test if len(word) > 2]\n",
    "test_filtered_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Write a list comprehension that selects only headings from the list stored in the variable `lines` and transforms them from uppercase to title case. Store the result in a new variable `headings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with words\n",
    "\n",
    "Usually, the unit of a text that we want to work with is not a line, but a word. Words are generally the smallest unit of a text that carries meaning (ignoring compounds for the moment). Splitting text into words is called “tokenisation,” as words are also called tokens in linguistics. And it is a surprisingly difficult task. In many languages—but not all, e.g. not in Chinese—words are usually separated by spaces. We can use that to our advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFACE',\n",
       " 'OF',\n",
       " 'SNORRE',\n",
       " 'STURLASON.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'I',\n",
       " 'have',\n",
       " 'had']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sample_text.split()\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works reasonable well, but not perfect: E.g., punctuation characters are still attached to the previous word. A way to improve the results is by using so-called “regular expressions.” These are rules that allow to search for patterns in texts. So one way of improving the result is by only looking at letters, not punctuation marks and other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFACE',\n",
       " 'OF',\n",
       " 'SNORRE',\n",
       " 'STURLASON',\n",
       " 'In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'I',\n",
       " 'have',\n",
       " 'had']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "words = re.findall('\\w+', sample_text)\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the usual Python functions are not sufficient, we imported additional functionality from another module. Python ships many modules by default, like the `re` module for regular expressions. Other modules come with extended Python version like Anaconda. And many more can be installed from the [Python Package Index](https://pypi.python.org/pypi). All functions from the `re` module can be called using the prefix \"`re.`\".\n",
    "\n",
    "The `findall` function looks for all occurrences of a given pattern in the text. The pattern `\\w+` says: Find a group of one or more (`+`) word characters (`\\w`, which is letters, digits, and \\_). Regular expressions are often helpful. We cannot go into detail here, but consult the [Python documentation](https://docs.python.org/3/howto/regex.html#regex-howto) if you want to learn more.\n",
    "\n",
    "Using this approach, we can then count the words in the text. Python has a handy function for this, we just need to import the `collections` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 36),\n",
       " ('of', 29),\n",
       " ('and', 27),\n",
       " ('in', 13),\n",
       " ('to', 11),\n",
       " ('a', 10),\n",
       " ('his', 9),\n",
       " ('be', 8),\n",
       " ('their', 7),\n",
       " ('have', 6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "words = re.findall('\\w+', text)\n",
    "counts = collections.Counter(words)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably not surprising that words like articles and prepositions appear quite often in the text. But add relatively little to understanding the text. So it makes sense to exclude them from the list. The simplest way to do so is to use stopword lists. The file `stopwords.txt` contains such a list for English, and we can read it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as stopwordfile:\n",
    "    stopwords = stopwordfile.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "\n",
    "stopwords[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation of the first stopwords list was necessary since the original stopwords still contain the line breaks character that is present in the text file.\n",
    "\n",
    "Now we can filter the words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 4),\n",
       " ('written', 3),\n",
       " ('poem', 3),\n",
       " ('chiefs', 3),\n",
       " ('raised', 3),\n",
       " ('reckoned', 3),\n",
       " ('son', 3),\n",
       " ('true', 3),\n",
       " ('family', 3),\n",
       " ('people', 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in words if not word in stopwords]\n",
    "counts = collections.Counter(words)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already helpful, but of course a simple frequency list is not the most advanced text analysis technique. If you are interested in learning about the more interesting, though also more complex things, I recommend the [tatom](https://de.dariah.eu/tatom/) tutorial at DARIAH."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
