{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Text\n",
    "\n",
    "Text in Python has the data type “string.” That means that text is just a sequence of characters. When analysing text programmatically, we are generally not interested in formatting, but just in the raw text itself. As a consequence, you cannot simply open a Word document or something like that in Python. The text format that we usually work with is `.txt`.\n",
    "\n",
    "Text can be stored in different encodings. These specify how the characters (e.g., “a”, but also “á” and even “道”) are converted to 0 and 1 for the computer. The most modern encoding is UTF-8, which follows the Unicode standard. You should make sure that your texts are always saved using UTF-8 in order to prevent issues when reading them in Python.\n",
    "\n",
    "That said, let’s look how to work with text in Python. In this directory, there is a file called “heimskringla_preface.txt”. It is the beginning of a Norwegian saga available from [Project Gutenberg](http://www.gutenberg.org/ebooks/598).\n",
    "\n",
    "In order to work with the text in python, we have to get it from the file stored on the computer’s harddrive, into a python variable that we can work with. It is good practice not to keep the file open all the time. Since we can do everything we want with the text once it is stored in a variable, we can close the file directly after reading its content. So the steps are simply:\n",
    "\n",
    "1. Open file.\n",
    "2. Read content.\n",
    "3. Close file.\n",
    "4. Analyse text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textfile = open('heimskringla_preface.txt')\n",
    "text = textfile.read()\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more elegant, but a bit less readable variant is to open the file just as long as we need it and let Python close it automatically afterwards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('heimskringla_preface.txt') as textfile:\n",
    "    text = textfile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `text` is now a large string, containing the complete content of the file. It wouldn’t make sense to print it completely, so we just peek into its beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3076"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PREFACE OF SNORRE STURLASON.\\n\\nIn this book I have had old stories written down, as I have heard\\nthem told by intelligent people, concerning chiefs who have have held\\ndominion in the northern countries'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = text[0:200]\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE OF SNORRE STURLASON.\n",
      "\n",
      "In this book I have had old stories written down, as I have heard\n",
      "them told by intelligent people, concerning chiefs who have have held\n",
      "dominion in the northern countries\n"
     ]
    }
   ],
   "source": [
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complete text is often difficult to work with, so we’d want to split it into smaller parts. A starting point might be lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFACE OF SNORRE STURLASON.',\n",
       " '',\n",
       " 'In this book I have had old stories written down, as I have heard',\n",
       " 'them told by intelligent people, concerning chiefs who have have held',\n",
       " 'dominion in the northern countries']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sample_text.splitlines()\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with text, we often have to clean them up. Some parts of this work require manual investigation, but many parts can be described as simple rules. These are tasks that are perfect for Python. In this case, we might decide to convert the headings from uppercase to the more usual title case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preface Of Snorre Sturlason.\n",
      "\n",
      "In this book I have had old stories written down, as I have heard\n",
      "them told by intelligent people, concerning chiefs who have have held\n",
      "dominion in the northern countries\n"
     ]
    }
   ],
   "source": [
    "for line in lines:\n",
    "    if line.isupper():\n",
    "        line = line.title()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we want to transform data this way. And instead of printing it, we want to store the result for further processing. One good way to express such data conversions in Python are so-called *list comprehensions*. They are rules that describe how to transforma a list of values into another list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = ['This', 'is', 'a', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THIS', 'IS', 'A', 'TEST']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transformed = [word.upper() for word in test]\n",
    "test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'test']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transformed = [word for word in test if len(word) > 2 ]\n",
    "test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'Test']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transformed = [word.title() for word in test if len(word) > 2]\n",
    "test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'Test']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transformed = [word.title() if len(word) > 2 else word.lower() for word in test]\n",
    "test_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Write a list comprehension that does the same thing as the `for`-loop above, but stores the transformed lines into a new variable `lines_fixed` instead of printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with words\n",
    "\n",
    "Usually, the unit of a text that we want to work with is not a line, but a word. Words are generally the smallest unit of a text that carries meaning (ignoring compounds for the moment). Splitting text into words is called “tokenisation,” as words are also called tokens in linguistics. And it is a surprisingly difficult task. In many languages—but not all, e.g. not in Chinese—words are usually separated by spaces. We can use that to our advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFACE',\n",
       " 'OF',\n",
       " 'SNORRE',\n",
       " 'STURLASON.',\n",
       " 'In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'I',\n",
       " 'have',\n",
       " 'had']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sample_text.split()\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works reasonable well, but not perfect: E.g., punctuation characters are still attached to the previous word. A way to improve the results is by using so-called “regular expressions.” These are rules that allow to search for patterns in texts. So one way of improving the result is by only looking at letters, not punctuation marks and other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PREFACE',\n",
       " 'OF',\n",
       " 'SNORRE',\n",
       " 'STURLASON',\n",
       " 'In',\n",
       " 'this',\n",
       " 'book',\n",
       " 'I',\n",
       " 'have',\n",
       " 'had']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "words = re.findall('\\w+', sample_text)\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the usual Python functions are not sufficient, we imported additional functionality from another module. Python ships many modules by default, like the `re` module for regular expressions. Other modules come with extended Python version like Anaconda. And many more can be installed from the [Python Package Index](https://pypi.python.org/pypi). All functions from the `re` module can be called using the prefix \"`re.`\".\n",
    "\n",
    "The `findall` function looks for all occurrences of a given pattern in the text. The pattern `\\w+` says: Find a group of one or more (`+`) word characters (`\\w`, which is letters, digits, and \\_). Regular expressions are often helpful. We cannot go into detail here, but consult the [Python documentation](https://docs.python.org/3/howto/regex.html#regex-howto) if you want to learn more.\n",
    "\n",
    "Using this approach, we can then count the words in the text. Python has a handy function for this, we just need to import the `collections` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 36),\n",
       " ('of', 29),\n",
       " ('and', 27),\n",
       " ('in', 13),\n",
       " ('to', 11),\n",
       " ('a', 10),\n",
       " ('his', 9),\n",
       " ('be', 8),\n",
       " ('their', 7),\n",
       " ('have', 6)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "words = re.findall('\\w+', text)\n",
    "counts = collections.Counter(words)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably not surprising that words like articles and prepositions appear quite often in the text. But add relatively little to understanding the text. So it makes sense to exclude them from the list. The simplest way to do so is to use stopword lists. The file `stopwords.txt` contains such a list for English, and we can read it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as stopwordfile:\n",
    "    stopwords = stopwordfile.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "\n",
    "stopwords[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation of the first stopwords list was necessary since the original stopwords still contain the line breaks character that is present in the text file.\n",
    "\n",
    "Now we can filter the words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('time', 4),\n",
       " ('King', 3),\n",
       " ('raised', 3),\n",
       " ('son', 3),\n",
       " ('people', 3),\n",
       " ('chiefs', 3),\n",
       " ('true', 3),\n",
       " ('written', 3),\n",
       " ('songs', 3),\n",
       " ('old', 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in words if not word in stopwords]\n",
    "counts = collections.Counter(words)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already helpful, but of course a simple frequency list is not the most advanced text analysis technique. If you are interested in learning about the more interesting, though also more complex things, I recommend the [tatom](https://de.dariah.eu/tatom/) tutorial at DARIAH."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
