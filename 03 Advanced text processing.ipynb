{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced text processing\n",
    "\n",
    "In order to find out more about a text, we often need to understand more about the text’s linguistic structure. In the previous example, we used regular expressions for identifying individual words, and a stopword list to identify content words. But very soon, linguistic tasks can get very complex, i.e. when trying to identify syntactic structure or identifying the persons a text is talking about.\n",
    "\n",
    "These tasks define a whole area at the intersection of linguistics and computer science called natural language processing (NLP). For Python, several packages for this kind of analysis are available, including the natural language toolkit [NLTK](http://www.nltk.org/) and [spaCy](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the spaCy module for natural language processing, we have to install it and download it’s model files for the English language:\n",
    "\n",
    "    conda install spacy\n",
    "    python -m spacy download en\n",
    "\n",
    "We can now activate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the example text from the previous lesson, we can test some of spaCy’s capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('heimskringla_preface.txt') as textfile:\n",
    "    text = textfile.read()\n",
    "\n",
    "# SpaCy gets confused by line breaks in the text, so we replace them by spaces.\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core functionality of spaCy can be accessed by creating a processed version of the text that holds information about the text’s words, sentences, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The now created document behaves like a list of words, but each word carries additional information. This way, we can identify the word class (the “part of speec” or “POS”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE - NOUN\n",
      "OF - ADP\n",
      "SNORRE - PROPN\n",
      "STURLASON - PROPN\n",
      ". - PUNCT\n",
      "  - SPACE\n",
      "In - ADP\n",
      "this - DET\n",
      "book - NOUN\n",
      "I - PRON\n",
      "have - VERB\n",
      "had - VERB\n",
      "old - ADJ\n",
      "stories - NOUN\n",
      "written - VERB\n",
      "down - PART\n",
      ", - PUNCT\n",
      "as - ADP\n",
      "I - PRON\n",
      "have - VERB\n"
     ]
    }
   ],
   "source": [
    "for word in doc[0:20]:\n",
    "    print(word.text, '-', word.pos_)  # Note: it’s `.pos_`, not `.pos`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the meaning of the individual tags, see [SpaCy’s documentation](https://spacy.io/api/annotation#pos-tagging).\n",
    "\n",
    "To access structural information other than words, we can use special properties of the document, e.g. for sentences.\n",
    "\n",
    "*Technical note:* The sentences are not a `list` object, but something called a “generator.” In contrast to lists, a generator is built dynamically. As a consequence, one cannot tell how many elements the generator will contain, thus the usual indexing does not work. To print only the first sentences, we have to convert the generator to a proper list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREFACE OF SNORRE STURLASON.  \n",
      "In this book I have had old stories written down, as I have heard them told by intelligent people, concerning chiefs who have have held dominion in the northern countries, and who spoke the Danish tongue; and also concerning some of their family branches, according to what has been told me.\n",
      "Some of this is found in ancient family registers, in which the pedigrees of kings and other personages of high birth are reckoned up, and part is written down after old songs and ballads which our forefathers had for their amusement.\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "for sentence in sentences[0:3]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the structural information to gain additional insights into the text’s statistical properties. E.g., we can find the most frequent nouns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('who', 4),\n",
       " ('time', 4),\n",
       " ('people', 3),\n",
       " ('chiefs', 3),\n",
       " ('family', 3),\n",
       " ('what', 3),\n",
       " ('songs', 3),\n",
       " ('poem', 3),\n",
       " ('son', 3),\n",
       " ('death', 3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "nouns = [w.text for w in doc if w.pos_ == 'NOUN']\n",
    "noun_counts = Counter(nouns)\n",
    "noun_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, different forms of the same word are countent as different words. Here, this affects mainly plural forms (\"chiefs\" vs \"chief\"), but for highly inflectional languages, this problem becomes more severe. Often, we want to count the base forms, or lemmas, instead of the inflected word forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('poem', 5),\n",
       " ('time', 5),\n",
       " ('chief', 4),\n",
       " ('who', 4),\n",
       " ('son', 4),\n",
       " ('people', 3),\n",
       " ('family', 3),\n",
       " ('what', 3),\n",
       " ('song', 3),\n",
       " ('skald', 3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_lemmas = [w.lemma_ for w in doc if w.pos_ == 'NOUN']\n",
    "noun_lemma_counts = Counter(noun_lemmas)\n",
    "noun_lemma_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the count of \"poem\" increased from 3 to 5. As we can easily see, this does indeed stem from plural forms now included in the count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poem', 'poem', 'poem', 'poems', 'poems']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.text for w in doc if w.lemma_ == 'poem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of filtering the words according to their word class, we can also build statistics about the distribution of the word classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 110),\n",
       " ('VERB', 98),\n",
       " ('ADP', 87),\n",
       " ('PUNCT', 68),\n",
       " ('DET', 63),\n",
       " ('ADJ', 59),\n",
       " ('PROPN', 49),\n",
       " ('CCONJ', 33),\n",
       " ('PRON', 20),\n",
       " ('ADV', 20),\n",
       " ('PART', 14),\n",
       " ('SPACE', 5),\n",
       " ('NUM', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = [w.pos_ for w in doc]\n",
    "pos_counter = Counter(pos_tags)\n",
    "pos_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more comple kind of analysis are named entities. This term refers to words—or groups of words—that refer to identifyable entities like persons, groups, or places. SpaCy identifies them for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danish - NORP\n",
      "Thjodolf of Hvin - PERSON\n",
      "Harald Harfager - PERSON\n",
      "Ynglingatal - WORK_OF_ART\n",
      "Olaf Geirstadalf - ORG\n",
      "King Halfdan - ORG\n",
      "thirty - CARDINAL\n",
      "Fjolner - ORG\n",
      "Yngvefrey - GPE\n",
      "Swedes - NORP\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents[0:10]:\n",
    "    print(ent.text, '-', ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can count how many entities of which category are found throughout the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PERSON', 10),\n",
       " ('GPE', 9),\n",
       " ('ORG', 5),\n",
       " ('NORP', 4),\n",
       " ('WORK_OF_ART', 2),\n",
       " ('CARDINAL', 1),\n",
       " ('DATE', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_tags = [e.label_ for e in doc.ents]\n",
    "ne_counter = Counter(ne_tags)\n",
    "ne_counter.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what these labels mean, see the [list of codes](https://spacy.io/api/annotation#named-entities) for named entities.\n",
    "\n",
    "Now this can help us to find all the persons mentioned in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thjodolf of Hvin',\n",
       " 'Harald Harfager',\n",
       " 'Eyvind Skaldaspiller',\n",
       " 'Earl Hakon',\n",
       " 'Thjodolf',\n",
       " 'Frey',\n",
       " 'Dan Milkillate',\n",
       " 'Harald Harfager',\n",
       " 'the King of Norway',\n",
       " 'Harald']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persons = [e.text for e in doc.ents if e.label_ == 'PERSON']\n",
    "persons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a starting point for a more content-oriented analysis, e.g. by identifying the words frequently associated with certain persons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
